<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

---
layout:     
title:      xgboost推导及常见问题
date:       2020-01-14
author:     wy
header-img: img/ML_BLOG/xgb-bg.jpg
catalog: true

tags:
    - 机器学习
---


## 一、知识点回顾
xgboost公式推导中用到的一些基础知识点。

### 1.1 集成学习
 **思想**：将多个机器学习模型组合起来使用，得到一个更强的模型。被组合的模型称为弱学习器，组合之后的模型称为强学习器。

 根据组合的策略不同，诞生了各种不同的算法：
     **bagging**：各个弱学习器之间没有依赖关系，如：随机森林。
     **boosting**：训练过程为阶梯状，基模型按次序一一进行训练，每轮改变训练数据权值或概率分布，如：AdaBoost，梯度提升，XGBoost。

下图为boosting算法的运行原理：
![](/img/ML_BLOG/XGB_1.png)
### 1.2 前向分布算法
> 引李航《统计学习方法》第八章，8.3.1前向分布算法。李航老师描述的前向分布算法，让我对GBDT，xgboost等这一类的boosting算法有了一个相对统一的认识。

考虑加法模型:   
$$f(x) = \sum_{m=1}^M\ \beta_m b(x;\gamma_m)$$   
其中$\beta$是基函数的系数，$b$是基函数，$\gamma$是基函数的参数。  

给定训练数据及损失函数$L(y,f(x))$的条件下，我们的目标为：加法模型$f(x)$的损失函数最小,即：   
$$min\sum_{i=1}^N\ L(y_i,\sum_{m=1}^M\ \beta_m b(x_i;\gamma_m))$$    (1)   
（N个样本，M个学习器）   
这是一个复杂的优化问题(_即有基函数的系数(集成模型的参数)，又有每个基函数的参数_)，前向分布算法解决这个问题的想法是：由于学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数(1).这样可以简化优化的复杂度。具体每步只需优化如下损失函数：   
$$min\sum_{i=1}^N\ L(y_i,\beta b(x_i;\gamma))$$       (2)  
(_这是一层的损失函数_)

**前向分布算法步骤**
**INPUT:**训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$；损失函数$L(y,f(x))$；基函数**集**${b(x;y)}$ ;
**OUTPUT**：加法模型$f(x)$
>(1)初始化：$f_0(x)=0$  
>(2)loop $m=1,2,...,M$
>     a)极小化损失函数:$$min\sum_{i=1}^N\ L(y_i,\beta b(x_i;\gamma))$$ 






