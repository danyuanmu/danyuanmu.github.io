<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

---
layout:     
title:      xgboost推导及常见问题
date:       2020-01-14
author:     wy
header-img: img/ML_BLOG/xgb-bg.jpg
catalog: true

tags:
    - 机器学习
---


## 一、知识点回顾
xgboost公式推导中用到的一些基础知识点。

### 1.1 集成学习
 **思想**：将多个机器学习模型组合起来使用，得到一个更强的模型。被组合的模型称为弱学习器，组合之后的模型称为强学习器。

 根据组合的策略不同，诞生了各种不同的算法：
     **bagging**：各个弱学习器之间没有依赖关系，如：随机森林。
     **boosting**：训练过程为阶梯状，基模型按次序一一进行训练，每轮改变训练数据权值或概率分布，如：AdaBoost，梯度提升，XGBoost。

下图为boosting算法的运行原理：
![](/img/ML_BLOG/XGB_1.png)
### 1.2 前向分布算法
> 引李航《统计学习方法》第八章，8.3.1前向分布算法。李航老师描述的前向分布算法，让我对GBDT，xgboost等这一类的boosting算法有了一个相对统一的认识。

考虑加法模型:   
<center>$$f(x) = \sum_{m=1}^M\ \beta_m b(x;\gamma_m)$$   </center>
其中$\beta$是基函数的系数，$b$是基函数，$\gamma$是基函数的参数。  

给定训练数据及损失函数$L(y,f(x))$的条件下，我们的目标为：加法模型$f(x)$的损失函数最小,即：   
<center>$$min\sum_{i=1}^N\ L(y_i,\sum_{m=1}^M\ \beta_m b(x_i;\gamma_m))$$  </center>   
（N个样本，M个学习器）   
这是一个复杂的优化问题(_即有基函数的系数(集成模型的参数)，又有每个基函数的参数_)，前向分布算法解决这个问题的想法是：由于学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数(1).这样可以简化优化的复杂度。具体每步只需优化如下损失函数：   
<center>$$min\sum_{i=1}^N\ L(y_i,\beta b(x_i;\gamma))$$  </center>
(_这是一层的损失函数_)

**前向分布算法步骤**  
**INPUT:**训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$；损失函数$L(y,f(x))$；基函数**集**${b(x;y)}$    
**OUTPUT**：加法模型$f(x)$   
**步骤**   
>(1)初始化：$f_0(x)=0$  
>(2)loop $m=1,2,...,M$
>>a)极小化损失函数：$$(\beta_m,\gamma_m)=arg min \sum_{i=1}^N\ L(y_i,f_m-1(x_i)+\beta b(x_i;\gamma))$$    (_得到当前层基函数的参数**$\gamma$**和系数**$\beta$**，其中上一层的基层模型$f_m-1(x_i)$是已知_)   
>>b)更新当前层的集成模型：$$f_m(x)=f_m-1(x)+\beta b(x_i;\gamma)$$    
>(3)得到加法模型：$$f(x)=f_M(x)=\sum_{m=1}^M\ \beta_m b(x;\gamma_m)$$

### 1.3 牛顿法
xgboost中用到的求极值的方法。**原理：**求解函数极值的数值优化（近似求解）算法，是使用**函数的泰勒展开式的前面几项**来寻找方程的根。   
根据微积分中的Fermat定理，函数在$x^*$处取得极值的必要条件是**梯度为0**，即：   
<center>$\bigtriangledown f'(x^*)=0$</center>   
对于复杂的非线性方程组，直接计算梯度，是很难的，因此大都采用**迭代法近似计算**。即从一个初始点，**反复使用某种规则（如：梯度）**从当前$x_k$ 移动至$x_(k+1)$，直至到达函数的极值点。
>**对多元函数在$x_0$处作一阶泰勒展开如下：**
>><center>$f(x)=f(x_0)+(x-x_0) \frac{f'(x_0)}{1}$+...</center>   
>>两边在$x_0$处求导：   
>><center>$f'(x)=f'(x_0)+ \frac{f''(x_0)}{1!} (x-x_0)+...$</center>   
>>令$f'(x_0)=0$,得到更新规则如下：   
>><center>$x=x_0-\frac{f'(x_0)}{f''(x_0)}$</center>










