<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

---
layout:     
title:      xgboost推导及常见问题
date:       2020-01-14
author:     wy
header-img: img/ML_BLOG/xgb_bg.jpg
catalog: true

tags:
    - 机器学习
---


## 一、知识点回顾
xgboost公式推导中用到的一些基础知识点。

### 1.1 集成学习
 **思想**：将多个机器学习模型组合起来使用，得到一个更强的模型。被组合的模型称为弱学习器，组合之后的模型称为强学习器。

 根据组合的策略不同，诞生了各种不同的算法：
     **bagging**：各个弱学习器之间没有依赖关系，如：随机森林。
     **boosting**：训练过程为阶梯状，基模型按次序一一进行训练，每轮改变训练数据权值或概率分布，如：AdaBoost，梯度提升，XGBoost。

下图为boosting算法的运行原理：
![](/img/ML_BLOG/XGB_1.png)
### 1.2 前向分布算法
> 引李航《统计学习方法》第八章，8.3.1前向分布算法。李航老师描述的前向分布算法，让我对GBDT，xgboost等这一类的boosting算法有了一个相对统一的认识。

考虑加法模型:   
<center>$$f(x) = \sum_{m=1}^M\ \beta_m b(x;\gamma_m)$$   </center>
其中$\beta$是基函数的系数，$b$是基函数，$\gamma$是基函数的参数。  

给定训练数据及损失函数$L(y,f(x))$的条件下，我们的目标为：加法模型$f(x)$的损失函数最小,即：   
<center>$$min\sum_{i=1}^N\ L(y_i,\sum_{m=1}^M\ \beta_m b(x_i;\gamma_m))$$  </center>   
（N个样本，M个学习器）   
这是一个复杂的优化问题(_即有基函数的系数(集成模型的参数)，又有每个基函数的参数_)，前向分布算法解决这个问题的想法是：由于学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数(1).这样可以简化优化的复杂度。具体每步只需优化如下损失函数：   
<center>$$min\sum_{i=1}^N\ L(y_i,\beta b(x_i;\gamma))$$  </center>
(_这是一层的损失函数_)

**前向分布算法步骤**  
**INPUT:**训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$；损失函数$L(y,f(x))$；基函数**集**${b(x;y)}$    
**OUTPUT**：加法模型$f(x)$   
**步骤**   
```
(1)初始化：$f_0(x)=0$     
(2)loop $m=1,2,...,M$   
   a)极小化损失函数：$$(\beta_m,\gamma_m)=arg min \sum_{i=1}^N\ L(y_i,f_m-1(x_i)+\beta b(x_i;\gamma))$$ (_得到当前层基函数的参数**$\gamma$**和系数**$\beta$**，其中上一层的基层模型$f_m-1(x_i)$是已知_)     
   b)更新当前层的集成模型：$$f_m(x)=f_m-1(x)+\beta b(x_i;\gamma)$$    
(3)得到加法模型：$$f(x)=f_M(x)=\sum_{m=1}^M\ \beta_m b(x;\gamma_m)$$   
```
### 1.3 牛顿法
xgboost中用到的求极值的方法。**原理：**求解函数极值的数值优化（近似求解）算法，是使用**函数的泰勒展开式的前面几项**来寻找方程的根。   
根据微积分中的Fermat定理，函数在$x^*$处取得极值的必要条件是**梯度为0**，即：   
<center>$\bigtriangledown f'(x^*)=0$</center>   
对于复杂的非线性方程组，直接计算梯度，是很难的，因此大都采用**迭代法近似计算**。即从一个初始点，**反复使用某种规则（如：梯度）**从当前$x_k$ 移动至$x_(k+1)$，直至到达函数的极值点。
**对多元函数在$x_0$处作一阶泰勒展开如下：**
<center>$f(x)=f(x_0)+(x-x_0) \frac{f'(x_0)}{1}$+...</center>   
两边在$x_0$处求导：   
<center>$f'(x)=f'(x_0)+ \frac{f''(x_0)}{1!} (x-x_0)+...$</center>   
令$f'(x_0)=0$,得到更新规则如下：   
<center>$x=x_0-\frac{f'(x_0)}{f''(x_0)}$</center>

## 二、xgboost推导
>下面的所有符号都与原论文统一，与上文无关。
### 2.1 模型   

**xgboost的集成模型描述如下：**   
<center>$$\hat{y_i}=\phi (x_i)=\sum_{k=1}^K\ f_k(x_i),f_k \in F$$</center>   

设有K个基模型(基模型为树模型)：   
　　`$f_k (x_i)$`为第i个样本在第k个基模型的得分；    
　　`$\hat{y_i}$` 为第i个样本在集成模型的**得分**。   
下图为score的计算方式：
![](/img/ML_BLOG/xgb_2.jpg){:height="70%" width="70%"}

### 2.2 损失函数简化
>这一步操作的主要目的：像前向分布算法模型中提到的，我们期望对xgboost的损失函数进行简化，以使在极小化损失函数的时候，**每一步只需学习一个基函数及其系数**。   
**xgboost的损失函数如下：**
![](/img/ML_BLOG/xgb_3.png)
    •  l是一个可微的凸损失函数，测量预测值和目标值的不同   
    •  Ω是正则项，用于抑制模型的复杂度防止过拟合   
    • T是叶子节点个数（对叶子节点数较多的树做出惩罚）   
w基模型的参数
 下面开始对损失函数进行简化，期望获得**只与当前的基模型相关的损失函数**。 
 按牛顿法的思想，讲损失函数$L(\phi)$泰勒展开到二阶:
![](/img/ML_BLOG/xgb_4.png)
其中









